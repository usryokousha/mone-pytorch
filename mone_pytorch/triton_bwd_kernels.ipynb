{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def get_1d_offset(size, n_prev_chunks):\n",
    "    return n_prev_chunks * size + tl.arange(0, size)\n",
    "\n",
    "@triton.jit\n",
    "def get_2d_offset(offs_0, offs_1, stride_0, stride_1=1):\n",
    "    return tl.expand_dims(offs_0, 1) * stride_0 + tl.expand_dims(offs_1, 0) * stride_1\n",
    "\n",
    "@triton.jit\n",
    "def get_1d_mask(offs, max):\n",
    "    return offs < max\n",
    "\n",
    "@triton.jit\n",
    "def get_2d_mask(offs_0, offs_1, max_0, max_1):\n",
    "    return (tl.expand_dims(offs_0, 1) < max_0 ) & (tl.expand_dims(offs_1, 0) < max_1)\n",
    "\n",
    "def get_cuda_autotune_config():\n",
    "    return [\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        # Good config for fp8 inputs.\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a bias to compute the output of a linear layer\n",
    "@triton.autotune(\n",
    "        configs=get_cuda_autotune_config(),\n",
    "        key=['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def nested_linear_expand_kernel(\n",
    "    x_ptr,\n",
    "    w_ptr,\n",
    "    o_ptr,\n",
    "    b_ptr,\n",
    "    mask_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_xm,\n",
    "    stride_xk,\n",
    "    stride_wk,\n",
    "    stride_wn,\n",
    "    stride_om,\n",
    "    stride_on,\n",
    "    stride_bias,\n",
    "    stride_mask,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(BLOCK_SIZE_M, pid_m)\n",
    "    rn = get_1d_offset(BLOCK_SIZE_N, pid_n)\n",
    "    rk = get_1d_offset(BLOCK_SIZE_K, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the m dimension\n",
    "    tmask = tl.load(mask_ptr + rm * stride_mask)\n",
    "    tmask = tl.expand_dims(tmask, 1)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for e_i in range(E): # e is the number of experts\n",
    "        # Get in_dim // 2^(e - e_i)\n",
    "        k_i = K >> (E - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_x = x_ptr + get_2d_offset(rm, rk, stride_xm, stride_xk)\n",
    "        offs_w = w_ptr + get_2d_offset(rk, rn, stride_wk, stride_wn)\n",
    "        \n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        x_mask_i = get_2d_mask(rm, rk, M, k_i)\n",
    "        w_mask_i = get_2d_mask(rk, rn, k_i, N)\n",
    "\n",
    "        # perform dot product for the current expert dimension\n",
    "        for _ in range(0, k_i, BLOCK_SIZE_K):\n",
    "            x = tl.load(offs_x, mask=t_mask_i & x_mask_i, other=0.0)\n",
    "            w = tl.load(offs_w, mask=w_mask_i, other=0.0)\n",
    "            acc += tl.dot(x, w)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_x += BLOCK_SIZE_K * stride_xk\n",
    "            offs_w += BLOCK_SIZE_K * stride_wk\n",
    "\n",
    "    # Add bias after accumulation\n",
    "    if b_ptr is not None:\n",
    "        offs_b = b_ptr + rn * stride_bias\n",
    "        b_mask = rn < N\n",
    "        b = tl.load(offs_b, mask=b_mask, other=0.0)\n",
    "        acc += b[None, :]\n",
    "\n",
    "    o = o_ptr + get_2d_offset(rm, rn, stride_om, stride_on)\n",
    "    mask = get_2d_mask(rm, rn, M, N)\n",
    "    tl.store(o, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_linear_expand(x, w, mask, b=None, experts=4):\n",
    "    assert x.shape[1] == w.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.is_contiguous(), \"Matrix x must be contiguous\"\n",
    "    M, K = x.shape\n",
    "    K, N = w.shape\n",
    "    # Allocate output\n",
    "    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n",
    "    # 1D launch kernel where each block gets its own program\n",
    "    grid = lambda meta: (\n",
    "        triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]),\n",
    "        triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]),\n",
    "    )\n",
    "    nested_linear_expand_kernel[grid](\n",
    "        x,\n",
    "        w,\n",
    "        output,\n",
    "        b,\n",
    "        mask,\n",
    "        M,\n",
    "        N,\n",
    "        K,\n",
    "        experts,\n",
    "        x.stride(0),\n",
    "        x.stride(1),\n",
    "        w.stride(0),\n",
    "        w.stride(1),\n",
    "        output.stride(0),\n",
    "        output.stride(1),\n",
    "        b.stride(0) if b is not None else None,\n",
    "        mask.stride(0),\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 64.,  64.,  64.,  ...,  64.,  64.,  64.],\n",
      "        [128., 128., 128.,  ..., 128., 128., 128.],\n",
      "        [128., 128., 128.,  ..., 128., 128., 128.],\n",
      "        [ 16.,  16.,  16.,  ...,  16.,  16.,  16.],\n",
      "        [ 16.,  16.,  16.,  ...,  16.,  16.,  16.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Test nested_linear_expand while keeping the mask fixed\n",
    "x = torch.ones((5, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "w = torch.ones((128, 256), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "bias = torch.zeros((256), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([2, 3, 3, 0, 0], dtype=torch.int32, device=\"cuda\")\n",
    "output = nested_linear_expand(x, w, mask, bias, experts=4)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the nested linear contraction\n",
    "@triton.autotune(\n",
    "        configs=get_cuda_autotune_config(),\n",
    "        key=['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def nested_linear_contract_kernel(\n",
    "    x_ptr,\n",
    "    w_ptr,\n",
    "    o_ptr,\n",
    "    b_ptr,\n",
    "    mask_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_xm,\n",
    "    stride_xk,\n",
    "    stride_wk,\n",
    "    stride_wn,\n",
    "    stride_om,\n",
    "    stride_on,\n",
    "    stride_bias,\n",
    "    stride_mask,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(BLOCK_SIZE_M, pid_m)\n",
    "    rn = get_1d_offset(BLOCK_SIZE_N, pid_n)\n",
    "    rk = get_1d_offset(BLOCK_SIZE_K, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the m dimension\n",
    "    tmask = tl.load(mask_ptr + rm * stride_mask)\n",
    "    tmask = tl.expand_dims(tmask, 1)\n",
    "\n",
    "    # mask for the current expert\n",
    "    x_mask_i = get_2d_mask(rm, rk, M, K)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for e_i in range(E): # e is the number of experts\n",
    "        # Get out_dim // 2^(e - e_i)\n",
    "        n_i = N >> (E - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_x = x_ptr + get_2d_offset(rm, rk, stride_xm, stride_xk)\n",
    "        offs_w = w_ptr + get_2d_offset(rk, rn, stride_wk, stride_wn)\n",
    "\n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        w_mask_i = get_2d_mask(rk, rn, K, n_i)\n",
    "\n",
    "        for _ in range(0, K, BLOCK_SIZE_K):\n",
    "            x = tl.load(offs_x, mask=t_mask_i & x_mask_i, other=0.0)\n",
    "            w = tl.load(offs_w, mask=w_mask_i, other=0.0)\n",
    "            acc += tl.dot(x, w)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_x += BLOCK_SIZE_K * stride_xk\n",
    "            offs_w += BLOCK_SIZE_K * stride_wk\n",
    "\n",
    "    # Add bias after accumulation\n",
    "    if b_ptr is not None:\n",
    "        offs_b = b_ptr + rn * stride_bias\n",
    "        b_mask = rn < N\n",
    "        b = tl.load(offs_b, mask=b_mask, other=0.0)\n",
    "        acc += b[None, :]\n",
    "\n",
    "    o = o_ptr + get_2d_offset(rm, rn, stride_om, stride_on)\n",
    "    mask = get_2d_mask(rm, rn, M, N)\n",
    "    tl.store(o, acc, mask=mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_linear_contract(x, w, mask, b=None, experts=4):\n",
    "    assert x.shape[1] == w.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.is_contiguous(), \"Matrix x must be contiguous\"\n",
    "    M, K = x.shape\n",
    "    K, N = w.shape\n",
    "    # Allocate output\n",
    "    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n",
    "    # 1D launch kernel where each block gets its own program\n",
    "    grid = lambda meta: (\n",
    "        triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]),\n",
    "        triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]),\n",
    "    )\n",
    "    nested_linear_contract_kernel[grid](\n",
    "        x,\n",
    "        w,\n",
    "        output,\n",
    "        b,\n",
    "        mask,\n",
    "        M,\n",
    "        N,\n",
    "        K,\n",
    "        experts,\n",
    "        x.stride(0),\n",
    "        x.stride(1),\n",
    "        w.stride(0),\n",
    "        w.stride(1),\n",
    "        output.stride(0),\n",
    "        output.stride(1),\n",
    "        b.stride(0) if b is not None else None,\n",
    "        mask.stride(0),\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256.],\n",
      "        [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test nested_linear_contract while keeping the mask fixed\n",
    "x = torch.ones((5, 256), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "w = torch.ones((256, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "bias = torch.zeros((128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([0, 1, 0, 0, 3], dtype=torch.int32, device=\"cuda\")\n",
    "output = nested_linear_contract(x, w, mask, bias, experts=4)\n",
    "print(output[:, :32])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a bias to compute the output of a linear layer\n",
    "@triton.autotune(\n",
    "        configs=get_cuda_autotune_config(),\n",
    "        key=['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def nested_linear_expand_dx_kernel(\n",
    "    dy_ptr,\n",
    "    wT_ptr,\n",
    "    dx_ptr,\n",
    "    mask_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_dym,\n",
    "    stride_dyk,\n",
    "    stride_wTk,\n",
    "    stride_wTn,\n",
    "    stride_dxm,\n",
    "    stride_dxn,\n",
    "    stride_mask,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(BLOCK_SIZE_M, pid_m)\n",
    "    rn = get_1d_offset(BLOCK_SIZE_N, pid_n)\n",
    "    rk = get_1d_offset(BLOCK_SIZE_K, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the m dimension\n",
    "    tmask = tl.load(mask_ptr + rm * stride_mask)\n",
    "    tmask = tl.expand_dims(tmask, 1)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for e_i in range(E): # e is the number of experts\n",
    "        # Get in_dim // 2^(e - e_i)\n",
    "        n_i = K >> (E - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_dy = dy_ptr + get_2d_offset(rm, rk, stride_dym, stride_dyk)\n",
    "        offs_wT = wT_ptr + get_2d_offset(rk, rn, stride_wTk, stride_wTn)  \n",
    "        \n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        dy_mask_i = get_2d_mask(rm, rk, M, K)\n",
    "        wT_mask_i = get_2d_mask(rk, rn, K, n_i)\n",
    "\n",
    "        # perform dot product for the current expert dimension\n",
    "        for _ in range(0, K, BLOCK_SIZE_K):\n",
    "            dy = tl.load(offs_dy, mask=t_mask_i & dy_mask_i, other=0.0)\n",
    "            wT = tl.load(offs_wT, mask=wT_mask_i, other=0.0)\n",
    "            acc += tl.dot(dy, wT)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_dy += BLOCK_SIZE_K * stride_dyk\n",
    "            offs_wT += BLOCK_SIZE_K * stride_wTn\n",
    "\n",
    "    dx = dx_ptr + get_2d_offset(rm, rn, stride_dxm, stride_dxn)\n",
    "    mask = get_2d_mask(rm, rn, M, N)\n",
    "    tl.store(dx, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_linear_expand_dx(dy, wT, mask, experts=4):\n",
    "    assert dy.shape[1] == wT.shape[0], \"Incompatible dimensions\"\n",
    "    assert dy.is_contiguous(), \"Matrix dy must be contiguous\"\n",
    "    M, K = dy.shape\n",
    "    K, N = wT.shape\n",
    "    # Allocate output\n",
    "    dx = torch.empty((M, N), device=dy.device, dtype=dy.dtype)\n",
    "    # 1D launch kernel where each block gets its own program\n",
    "    grid = lambda meta: (\n",
    "        triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]),\n",
    "        triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]),\n",
    "    )\n",
    "    nested_linear_expand_dx_kernel[grid](\n",
    "        dy,\n",
    "        wT,\n",
    "        dx,\n",
    "        mask,\n",
    "        M,\n",
    "        N,\n",
    "        K,\n",
    "        experts,\n",
    "        dy.stride(0),\n",
    "        dy.stride(1),\n",
    "        wT.stride(0),\n",
    "        wT.stride(1),\n",
    "        dx.stride(0),\n",
    "        dx.stride(1),\n",
    "        mask.stride(0),\n",
    "    )\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test nested_linear_expand_dx while keeping the mask fixed\n",
    "dy = torch.ones((5, 256), device=\"cuda\", dtype=torch.float16, requires_grad=True) / 256\n",
    "wT = torch.ones((256, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([1, 3, 3, 1, 2], dtype=torch.int32, device=\"cuda\")\n",
    "output = nested_linear_expand_dx(dy, wT, mask, experts=4)\n",
    "print(output[-1])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a bias to compute the output of a linear layer\n",
    "@triton.autotune(\n",
    "        configs=get_cuda_autotune_config(),\n",
    "        key=['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def nested_linear_expand_dw_kernel(\n",
    "    dyT_ptr,\n",
    "    x_ptr,\n",
    "    dw_ptr,\n",
    "    mask_ptr,\n",
    "    dbias_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_dyTm,\n",
    "    stride_dyTk,\n",
    "    stride_xk,\n",
    "    stride_xn,\n",
    "    stride_dwm,\n",
    "    stride_dwn,\n",
    "    stride_mask,\n",
    "    stride_bias,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(BLOCK_SIZE_M, pid_m)\n",
    "    rn = get_1d_offset(BLOCK_SIZE_N, pid_n)\n",
    "    rk = get_1d_offset(BLOCK_SIZE_K, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the k dimension\n",
    "    tmask = tl.load(mask_ptr + rk * stride_mask)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    acc_bias = tl.zeros((BLOCK_SIZE_M, ), dtype=tl.float32)\n",
    "    for e_i in range(E): # e is the number of experts\n",
    "        # Get out_dim // 2^(e - e_i)\n",
    "        n_i = N >> (E - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_dyT = dyT_ptr + get_2d_offset(rm, rk, stride_dyTm, stride_dyTk)\n",
    "        offs_x = x_ptr + get_2d_offset(rk, rn, stride_xk, stride_xn)\n",
    "        \n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        dyT_mask_i = get_2d_mask(rm, rk, M, K)\n",
    "        x_mask_i = get_2d_mask(rk, rn, K, n_i)\n",
    "\n",
    "        # perform dot product for the current expert dimension\n",
    "        for _ in range(0, K, BLOCK_SIZE_K):\n",
    "            dyT = tl.load(offs_dyT, mask=tl.expand_dims(t_mask_i, 0) & dyT_mask_i, other=0.0)\n",
    "            x = tl.load(offs_x, mask=tl.expand_dims(t_mask_i, 1) & x_mask_i, other=0.0)\n",
    "            acc += tl.dot(dyT, x)\n",
    "            if dbias_ptr is not None:\n",
    "                acc_bias += tl.sum(dyT, axis=1)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_dyT += BLOCK_SIZE_K * stride_dyTk\n",
    "            offs_x += BLOCK_SIZE_K * stride_xn\n",
    "\n",
    "    if dbias_ptr is not None:\n",
    "        offs_dbias = dbias_ptr + rm * stride_bias\n",
    "        dbias_mask = rm < M\n",
    "        tl.store(offs_dbias, acc_bias, mask=dbias_mask)\n",
    "\n",
    "    dw = dw_ptr + get_2d_offset(rm, rn, stride_dwm, stride_dwn)\n",
    "    mask = get_2d_mask(rm, rn, M, N)\n",
    "    tl.store(dw, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_linear_expand_dw(dyT, x, mask, bias=False, experts=4):\n",
    "    assert dyT.shape[1] == x.shape[0], \"Incompatible dimensions\"\n",
    "    assert dyT.is_contiguous(), \"Matrix dyT must be contiguous\"\n",
    "    M, K = dyT.shape\n",
    "    K, N = x.shape\n",
    "    # Allocate output\n",
    "    dw = torch.empty((M, N), device=dyT.device, dtype=dyT.dtype)\n",
    "    dbias = None\n",
    "    if bias:\n",
    "        dbias = torch.empty((M,), device=dyT.device, dtype=dyT.dtype)\n",
    "    grid = lambda meta: (\n",
    "        triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]),\n",
    "        triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]),\n",
    "    )\n",
    "    nested_linear_expand_dw_kernel[grid](\n",
    "        dyT,\n",
    "        x,\n",
    "        dw,\n",
    "        mask,\n",
    "        dbias,\n",
    "        M,\n",
    "        N,\n",
    "        K,\n",
    "        experts,\n",
    "        dyT.stride(0),\n",
    "        dyT.stride(1),\n",
    "        x.stride(0),\n",
    "        x.stride(1),\n",
    "        dw.stride(0),\n",
    "        dw.stride(1),\n",
    "        mask.stride(0),\n",
    "        dbias.stride(0) if bias else None,\n",
    "    )\n",
    "    return dw, dbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0195, 0.0195, 0.0195,  ..., 0.0078, 0.0078, 0.0078],\n",
      "        [0.0195, 0.0195, 0.0195,  ..., 0.0078, 0.0078, 0.0078],\n",
      "        [0.0195, 0.0195, 0.0195,  ..., 0.0078, 0.0078, 0.0078],\n",
      "        ...,\n",
      "        [0.0195, 0.0195, 0.0195,  ..., 0.0078, 0.0078, 0.0078],\n",
      "        [0.0195, 0.0195, 0.0195,  ..., 0.0078, 0.0078, 0.0078],\n",
      "        [0.0195, 0.0195, 0.0195,  ..., 0.0078, 0.0078, 0.0078]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "None\n",
      "torch.Size([256, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test nested_linear_contract_dx while keeping the mask fixed\n",
    "dyT = torch.ones((256, 5), device=\"cuda\", dtype=torch.float16, requires_grad=True) / 256\n",
    "x = torch.ones((5, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([1, 3, 3, 1, 2], dtype=torch.int32, device=\"cuda\")\n",
    "dw, dbias = nested_linear_expand_dw(dyT, x, mask, bias=False, experts=4)\n",
    "print(dw)\n",
    "print(dbias)\n",
    "print(dw.shape)\n",
    "if dbias is not None:\n",
    "    print(dbias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a bias to compute the output of a linear layer\n",
    "@triton.autotune(\n",
    "        configs=get_cuda_autotune_config(),\n",
    "        key=['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def nested_linear_contract_dx_kernel(\n",
    "    dy_ptr,\n",
    "    wT_ptr,\n",
    "    dx_ptr,\n",
    "    mask_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_dym,\n",
    "    stride_dyk,\n",
    "    stride_wTk,\n",
    "    stride_wTn,\n",
    "    stride_dxm,\n",
    "    stride_dxn,\n",
    "    stride_mask,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(BLOCK_SIZE_M, pid_m)\n",
    "    rn = get_1d_offset(BLOCK_SIZE_N, pid_n)\n",
    "    rk = get_1d_offset(BLOCK_SIZE_K, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the m dimension\n",
    "    tmask = tl.load(mask_ptr + rm * stride_mask)\n",
    "    tmask = tl.expand_dims(tmask, 1)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    \n",
    "    for e_i in range(E): # e is the number of experts\n",
    "        # Get in_dim // 2^(e - e_i)\n",
    "        k_i = K >> (E - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_dy = dy_ptr + get_2d_offset(rm, rk, stride_dym, stride_dyk)\n",
    "        offs_wT = wT_ptr + get_2d_offset(rk, rn, stride_wTk, stride_wTn)\n",
    "        \n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        dy_mask_i = get_2d_mask(rm, rk, M, k_i)\n",
    "        wT_mask_i = get_2d_mask(rk, rn, k_i, N)\n",
    "\n",
    "        # perform dot product for the current expert dimension\n",
    "        for _ in range(0, k_i, BLOCK_SIZE_K):\n",
    "            dy = tl.load(offs_dy, mask=t_mask_i & dy_mask_i, other=0.0)\n",
    "            wT = tl.load(offs_wT, mask=wT_mask_i, other=0.0)\n",
    "            acc += tl.dot(dy, wT)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_dy += BLOCK_SIZE_K * stride_dyk\n",
    "            offs_wT += BLOCK_SIZE_K * stride_wTn\n",
    "\n",
    "    dx = dx_ptr + get_2d_offset(rm, rn, stride_dxm, stride_dxn)\n",
    "    mask = get_2d_mask(rm, rn, M, N)\n",
    "    tl.store(dx, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_linear_contract_dx(dy, wT, mask, experts=4):\n",
    "    assert dy.shape[1] == wT.shape[0], \"Incompatible dimensions\"\n",
    "    assert dy.is_contiguous(), \"Matrix dy must be contiguous\"\n",
    "    M, N = dy.shape\n",
    "    K, N = wT.shape\n",
    "    # Allocate output\n",
    "    dx = torch.empty((M, N), device=dy.device, dtype=dy.dtype)\n",
    "    grid = lambda meta: (\n",
    "        triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]),\n",
    "        triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]),\n",
    "    )\n",
    "    nested_linear_contract_dx_kernel[grid](\n",
    "        dy,\n",
    "        wT,\n",
    "        dx,\n",
    "        mask,\n",
    "        M,\n",
    "        N,\n",
    "        K,\n",
    "        experts,\n",
    "        dy.stride(0),\n",
    "        dy.stride(1),\n",
    "        wT.stride(0),\n",
    "        wT.stride(1),\n",
    "        dx.stride(0),\n",
    "        dx.stride(1),\n",
    "        mask.stride(0),\n",
    "    )\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250,\n",
      "         0.1250, 0.1250],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500,\n",
      "         0.2500, 0.2500],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000]], device='cuda:0', dtype=torch.float16)\n",
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test nested_linear_contract_dx while keeping the mask fixed\n",
    "dy = torch.ones((5, 256), device=\"cuda\", dtype=torch.float16, requires_grad=True) / 256\n",
    "wT = torch.ones((256, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([0, 3, 3, 1, 2], dtype=torch.int32, device=\"cuda\")\n",
    "output = nested_linear_contract_dx(dy, wT, mask, experts=4)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a bias to compute the output of a linear layer\n",
    "@triton.autotune(\n",
    "        configs=get_cuda_autotune_config(),\n",
    "        key=['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def nested_linear_contract_dw_kernel(\n",
    "    dyT_ptr,\n",
    "    x_ptr,\n",
    "    dw_ptr,\n",
    "    mask_ptr,\n",
    "    dbias_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_dyTm,\n",
    "    stride_dyTk,\n",
    "    stride_xk,\n",
    "    stride_xn,\n",
    "    stride_dwm,\n",
    "    stride_dwn,\n",
    "    stride_mask,\n",
    "    stride_bias,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(BLOCK_SIZE_M, pid_m)\n",
    "    rn = get_1d_offset(BLOCK_SIZE_N, pid_n)\n",
    "    rk = get_1d_offset(BLOCK_SIZE_K, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the k dimension\n",
    "    tmask = tl.load(mask_ptr + rk * stride_mask)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    acc_bias = tl.zeros((BLOCK_SIZE_M, ), dtype=tl.float32)\n",
    "    for e_i in range(E): # e is the number of experts\n",
    "        # Get out_dim // 2^(e - e_i)\n",
    "        m_i = M >> (E - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_dyT = dyT_ptr + get_2d_offset(rm, rk, stride_dyTm, stride_dyTk)\n",
    "        offs_x = x_ptr + get_2d_offset(rk, rn, stride_xk, stride_xn)\n",
    "        \n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        dyT_mask_i = get_2d_mask(rm, rk, m_i, K)\n",
    "        x_mask_i = get_2d_mask(rk, rn, K, N)\n",
    "\n",
    "        # perform dot product for the current expert dimension\n",
    "        for _ in range(0, K, BLOCK_SIZE_K):\n",
    "            dyT = tl.load(offs_dyT, mask=tl.expand_dims(t_mask_i, 0) & dyT_mask_i, other=0.0)\n",
    "            x = tl.load(offs_x, mask=tl.expand_dims(t_mask_i, 1) & x_mask_i, other=0.0)\n",
    "            acc += tl.dot(dyT, x)\n",
    "            if dbias_ptr is not None:\n",
    "                acc_bias += tl.sum(dyT, axis=1)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_dyT += BLOCK_SIZE_K * stride_dyTk\n",
    "            offs_x += BLOCK_SIZE_K * stride_xn\n",
    "\n",
    "    if dbias_ptr is not None:\n",
    "        offs_dbias = dbias_ptr + rm * stride_bias\n",
    "        dbias_mask = rm < M\n",
    "        tl.store(offs_dbias, acc_bias, mask=dbias_mask)\n",
    "\n",
    "    dw = dw_ptr + get_2d_offset(rm, rn, stride_dwm, stride_dwn)\n",
    "    mask = get_2d_mask(rm, rn, M, N)\n",
    "    tl.store(dw, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_linear_contract_dw(dyT, x, mask, bias=False, experts=4):\n",
    "    assert dyT.shape[1] == x.shape[0], \"Incompatible dimensions\"\n",
    "    assert dyT.is_contiguous(), \"Matrix dyT must be contiguous\"\n",
    "    M, N = dyT.shape\n",
    "    K, N = x.shape\n",
    "    # Allocate output\n",
    "    dw = torch.empty((M, N), device=dyT.device, dtype=dyT.dtype)\n",
    "    dbias = None\n",
    "    if bias:\n",
    "        dbias = torch.empty((M,), device=dyT.device, dtype=dyT.dtype)\n",
    "    grid = lambda meta: (\n",
    "        triton.cdiv(M, meta[\"BLOCK_SIZE_M\"]),\n",
    "        triton.cdiv(N, meta[\"BLOCK_SIZE_N\"]),\n",
    "    )\n",
    "    nested_linear_contract_dw_kernel[grid](\n",
    "        dyT,\n",
    "        x,\n",
    "        dw,\n",
    "        mask,\n",
    "        dbias,\n",
    "        M,\n",
    "        N,\n",
    "        K,\n",
    "        experts,\n",
    "        dyT.stride(0),\n",
    "        dyT.stride(1),\n",
    "        x.stride(0),\n",
    "        x.stride(1),\n",
    "        dw.stride(0),\n",
    "        dw.stride(1),\n",
    "        mask.stride(0),\n",
    "        dbias.stride(0) if bias else None,\n",
    "    )\n",
    "    return dw, dbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0195, 0.0195, 0.0195,  ..., 0.0195, 0.0195, 0.0195],\n",
      "        [0.0195, 0.0195, 0.0195,  ..., 0.0195, 0.0195, 0.0195],\n",
      "        [0.0195, 0.0195, 0.0195,  ..., 0.0195, 0.0195, 0.0195],\n",
      "        ...,\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch.Size([256, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test nested_linear_contract_dw while keeping the mask fixed\n",
    "dyT = torch.ones((256, 5), device=\"cuda\", dtype=torch.float16, requires_grad=True) / 256\n",
    "x = torch.ones((5, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([0, 1, 3, 1, 2], dtype=torch.int32, device=\"cuda\")\n",
    "dW, dbias = nested_linear_contract_dw(dyT, x, mask, bias=False, experts=4)\n",
    "print(dW)\n",
    "print(dW.shape)\n",
    "if dbias is not None:\n",
    "    print(dbias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedLinearExpand(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w, mask, bias=None, experts=4):\n",
    "        ctx.experts = experts\n",
    "        ctx.bias = False if bias is None else True\n",
    "        ctx.save_for_backward(x, w, mask)\n",
    "        return nested_linear_expand(x, w.transpose(0, 1), mask, bias, experts)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, w, mask = ctx.saved_tensors\n",
    "        experts = ctx.experts\n",
    "        bias = ctx.bias\n",
    "        dx = nested_linear_expand_dx(grad_output, w, mask, experts=experts)\n",
    "        dw, dbias = nested_linear_expand_dw(\n",
    "            grad_output.transpose(0, 1), x, mask, bias=bias, experts=experts\n",
    "        )\n",
    "        return dx, dw, None, dbias, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedLinearContract(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w, mask, bias=None, experts=4):\n",
    "        ctx.experts = experts\n",
    "        ctx.bias = False if bias is None else True\n",
    "        ctx.save_for_backward(x, w, mask)\n",
    "        return nested_linear_contract(x, w.transpose(0, 1), mask, bias, experts)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, w, mask = ctx.saved_tensors\n",
    "        experts = ctx.experts\n",
    "        bias = ctx.bias\n",
    "        dy = nested_linear_contract_dx(grad_output, w, mask, experts=experts)\n",
    "        dw, dbias = nested_linear_contract_dw(\n",
    "            grad_output.transpose(0, 1), x, mask, bias=bias, experts=experts\n",
    "        )\n",
    "        return dy, dw, None, dbias, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
