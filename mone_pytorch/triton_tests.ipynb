{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def get_1d_offset(size, n_prev_chunks):\n",
    "    return n_prev_chunks * size + tl.arange(0, size)\n",
    "\n",
    "@triton.jit\n",
    "def get_2d_offset(offs_0, offs_1, stride_0, stride_1=1):\n",
    "    return tl.expand_dims(offs_0, 1) * stride_0 + tl.expand_dims(offs_1, 0) * stride_1\n",
    "\n",
    "@triton.jit\n",
    "def get_1d_mask(offs, max):\n",
    "    return offs < max\n",
    "\n",
    "@triton.jit\n",
    "def get_2d_mask(offs_0, offs_1, max_0, max_1):\n",
    "    return (tl.expand_dims(offs_0, 1) < max_0 ) & (tl.expand_dims(offs_1, 0) < max_1)\n",
    "\n",
    "def get_cuda_autotune_config():\n",
    "    return [\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        # Good config for fp8 inputs.\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def matmul_naive_k(\n",
    "    a_ptr,\n",
    "    b_ptr,\n",
    "    c_ptr,\n",
    "    m,\n",
    "    n,\n",
    "    k,\n",
    "    stride_am,\n",
    "    stride_ak,\n",
    "    stride_bk,\n",
    "    stride_bn,\n",
    "    stride_cm,\n",
    "    stride_cn,\n",
    "    bm: tl.constexpr,\n",
    "    bn: tl.constexpr,\n",
    "    bk: tl.constexpr,\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    # get number of chunks for each dimension\n",
    "    rm = get_1d_offset(bm, pid_m)\n",
    "    rn = get_1d_offset(bn, pid_n)\n",
    "    rk = get_1d_offset(bk, 0) # K will always start from 0\n",
    "\n",
    "    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)\n",
    "    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)\n",
    "\n",
    "    acc = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    for _ in range(0, k, bk):\n",
    "        a = tl.load(offs_a, mask=get_2d_mask(rm, rk, m, k), other=0.0)\n",
    "        b = tl.load(offs_b, mask=get_2d_mask(rk, rn, k, n), other=0.0)\n",
    "        acc += tl.dot(a, b, allow_tf32=False)\n",
    "        offs_a += bk * stride_ak\n",
    "        offs_b += bk * stride_bk\n",
    "\n",
    "    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)\n",
    "    mask = get_2d_mask(rm, rn, m, n)\n",
    "    tl.store(c, acc, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def matmul(a, b, matmul_k_fn, bs=16, group_sz=None):\n",
    "    assert a.shape[1] == b.shape[0], \"matrix dims not compatible for matmul\"\n",
    "    (m, k), (_, n) = a.shape, b.shape\n",
    "    c = torch.empty((m, n), device=a.device, dtype=torch.float16)\n",
    "    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))\n",
    "    group_sz = {} if group_sz is None else {\"group_sz\":group_sz} # not used in naive_matmul, but will be in grouped_matmul further below \n",
    "    matmul_k_fn[grid](\n",
    "        a, b, c,\n",
    "        m, n, k,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        bm=bs, bn=bs, bk=bs, # Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile\n",
    "        **group_sz\n",
    "    )\n",
    "    return c\n",
    "\n",
    "naive_matmul = partial(matmul, matmul_k_fn=matmul_naive_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((3, 4), dtype=torch.float32, device=\"cuda\")\n",
    "b = torch.ones((4, 5), dtype=torch.float32, device=\"cuda\")\n",
    "naive_matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "triton_output = naive_matmul(a, b)\n",
    "torch_output = torch.matmul(a, b)\n",
    "if torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def swizzle_k(x_ptr, z_ptr, group_sz: tl.constexpr):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m_, pid_n_ = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU\n",
    "    \n",
    "    # get offsets aligned to original program ids\n",
    "    offs_m = get_1d_offset(1, n_prev_chunks=pid_m)\n",
    "    offs_n = get_1d_offset(1, n_prev_chunks=pid_n)\n",
    "    \n",
    "    # get mask for original program ids\n",
    "    offs = get_2d_offset(offs_m, offs_n, stride_0=num_pid_n)\n",
    "    mask = get_2d_mask(offs_m, offs_n, max_0=num_pid_m, max_1=num_pid_n )\n",
    "\n",
    "    # get offsets aligned to swizzled program ids\n",
    "    offs_sw_m = get_1d_offset(1, n_prev_chunks=pid_m_)\n",
    "    offs_sw_n = get_1d_offset(1, n_prev_chunks=pid_n_)\n",
    "    \n",
    "    # get mask for swizzled program ids\n",
    "    offs_sw = get_2d_offset(offs_sw_m, offs_sw_n, stride_0=num_pid_n)\n",
    "    mask_sw = get_2d_mask(offs_sw_m, offs_sw_n, max_0=num_pid_m, max_1=num_pid_n)\n",
    "    \n",
    "    # load using original program ids and store using swizzled program ids\n",
    "    x = tl.load(x_ptr + offs, mask=mask)\n",
    "    tl.store(z_ptr + offs_sw, x, mask=mask_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_m, blocks_n = 5, 4\n",
    "x = torch.arange(blocks_m * blocks_n, device='cuda').view(blocks_m, blocks_n)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = -torch.torch.ones_like(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3,  6,  9],\n",
       "        [ 1,  4,  7, 10],\n",
       "        [ 2,  5,  8, 11],\n",
       "        [12, 14, 16, 18],\n",
       "        [13, 15, 17, 19]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swizzle_k[(blocks_m, blocks_n)](x, z, group_sz=3)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def grouped_matmul_k(\n",
    "    a_ptr,\n",
    "    b_ptr,\n",
    "    c_ptr,\n",
    "    m,\n",
    "    n,\n",
    "    k,\n",
    "    stride_am,\n",
    "    stride_ak,\n",
    "    stride_bk,\n",
    "    stride_bn,\n",
    "    stride_cm,\n",
    "    stride_cn,\n",
    "    bm: tl.constexpr,\n",
    "    bn: tl.constexpr,\n",
    "    bk: tl.constexpr,\n",
    "    group_sz: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(bm, pid_m)\n",
    "    rn = get_1d_offset(bn, pid_n)\n",
    "    rk = get_1d_offset(bk, 0) # K will always start from 0\n",
    "\n",
    "    # relevant offsets for a and b\n",
    "    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)\n",
    "    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    for _ in range(0, k, bk):\n",
    "        a = tl.load(offs_a, mask=get_2d_mask(rm, rk, m, k), other=0.0)\n",
    "        b = tl.load(offs_b, mask=get_2d_mask(rk, rn, k, n), other=0.0)\n",
    "        acc += tl.dot(a, b, allow_tf32=False)\n",
    "        # update offsets for next iteration \n",
    "        offs_a += bk * stride_ak\n",
    "        offs_b += bk * stride_bk\n",
    "\n",
    "    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)\n",
    "    mask = get_2d_mask(rm, rn, m, n)\n",
    "    tl.store(c, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_matmul = partial(matmul, matmul_k_fn=grouped_matmul_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((3, 4), dtype=torch.float32, device=\"cuda\")\n",
    "b = torch.ones((4, 5), dtype=torch.float32, device=\"cuda\")\n",
    "grouped_matmul(a, b, group_sz=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def masked_matmul_k(\n",
    "    a_ptr,\n",
    "    b_ptr,\n",
    "    c_ptr,\n",
    "    mask_ptr,\n",
    "    m,\n",
    "    n,\n",
    "    k,\n",
    "    e,\n",
    "    stride_am,\n",
    "    stride_ak,\n",
    "    stride_bk,\n",
    "    stride_bn,\n",
    "    stride_cm,\n",
    "    stride_cn,\n",
    "    stride_mask,\n",
    "    bm: tl.constexpr,\n",
    "    bn: tl.constexpr,\n",
    "    bk: tl.constexpr,\n",
    "    group_sz: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(bm, pid_m)\n",
    "    rn = get_1d_offset(bn, pid_n)\n",
    "    rk = get_1d_offset(bk, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the m dimension\n",
    "    tmask = tl.load(mask_ptr + rm * stride_mask)\n",
    "    tmask = tl.expand_dims(tmask, 1)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((bm, bn), dtype=tl.float32)\n",
    "    for e_i in range(e): # e is the number of experts\n",
    "        # Get in_dim // 2^(e - e_i)\n",
    "        k_i = k >> (e - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)\n",
    "        offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)\n",
    "        \n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        a_mask_i = get_2d_mask(rm, rk, m, k_i)\n",
    "        b_mask_i = get_2d_mask(rk, rn, k_i, n)\n",
    "\n",
    "        # perform dot product for the current expert dimension\n",
    "        for _ in range(0, k_i, bk):\n",
    "            a = tl.load(offs_a, mask=t_mask_i & a_mask_i, other=0.0)\n",
    "            b = tl.load(offs_b, mask=b_mask_i, other=0.0)\n",
    "            acc += tl.dot(a, b, allow_tf32=True)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_a += bk * stride_ak\n",
    "            offs_b += bk * stride_bk\n",
    "\n",
    "    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)\n",
    "    mask = get_2d_mask(rm, rn, m, n)\n",
    "    tl.store(c, acc, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_matmul(a, b, mask, experts=4, bs=16,group_sz=None):\n",
    "    assert a.shape[1] == b.shape[0], \"matrix dims not compatible for matmul\"\n",
    "    assert mask.shape[0] == a.shape[0], \"mask dims not compatible for matmul\"\n",
    "    (m, k), (_, n) = a.shape, b.shape\n",
    "    e = experts\n",
    "    c = torch.empty((m, n), device=a.device, dtype=torch.float32)\n",
    "    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))\n",
    "    group_sz = {} if group_sz is None else {\"group_sz\":group_sz} # not used in naive_matmul, but will be in grouped_matmul further below \n",
    "    masked_matmul_k[grid](\n",
    "        a, b, c, mask,\n",
    "        m, n, k, e,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        mask.stride(0),\n",
    "        bm=bs, bn=bs, bk=bs, # Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile\n",
    "        **group_sz\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4., 4.]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((3, 4), dtype=torch.float32, device=\"cuda\")\n",
    "b = torch.ones((4, 5), dtype=torch.float32, device=\"cuda\")\n",
    "experts = 1\n",
    "mask = torch.tensor([0, 0, 0], dtype=torch.int32, device=\"cuda\")\n",
    "masked_matmul(a, b, mask, experts, bs=16, group_sz=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains 128: True\n",
      "contains 64: True\n",
      "contains 32: True\n",
      "shape: torch.Size([5, 256])\n",
      "tensor([64., 64., 64., 64., 64., 64., 64., 64., 64., 64.], device='cuda:0')\n",
      "tensor([32., 32., 32., 32., 32., 32., 32., 32., 32., 32.], device='cuda:0')\n",
      "tensor([128., 128., 128., 128., 128., 128., 128., 128., 128., 128.],\n",
      "       device='cuda:0')\n",
      "tensor([16., 16., 16., 16., 16., 16., 16., 16., 16., 16.], device='cuda:0')\n",
      "tensor([64., 64., 64., 64., 64., 64., 64., 64., 64., 64.], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 64.,  64.,  64.,  ...,  64.,  64.,  64.],\n",
       "        [ 32.,  32.,  32.,  ...,  32.,  32.,  32.],\n",
       "        [128., 128., 128.,  ..., 128., 128., 128.],\n",
       "        [ 16.,  16.,  16.,  ...,  16.,  16.,  16.],\n",
       "        [ 64.,  64.,  64.,  ...,  64.,  64.,  64.]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((5, 128), dtype=torch.float32, device=\"cuda\", requires_grad=True)\n",
    "b = torch.ones((128, 256), dtype=torch.float32, device=\"cuda\", requires_grad=True)\n",
    "experts = 4\n",
    "mask = torch.tensor([2, 1, 3, 0, 2], dtype=torch.int32, device=\"cuda\")\n",
    "output = masked_matmul(a, b, mask, experts, bs=16, group_sz=4)\n",
    "print(f\"contains 128: {torch.any(output == 128)}\")\n",
    "print(f\"contains 64: {torch.any(output == 64)}\")\n",
    "print(f\"contains 32: {torch.any(output == 32)}\")\n",
    "print(f\"shape: {output.shape}\")\n",
    "print(output[0, :10])\n",
    "print(output[1, :10])\n",
    "print(output[2, :10])\n",
    "print(output[3, :10])\n",
    "print(output[4, :10])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
      "        0.2000], device='cuda:0')\n",
      "tensor([0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
      "        0.0039], device='cuda:0')\n",
      "tensor([0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n",
      "        0.0039], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import linear\n",
    "bias = torch.ones((256), dtype=torch.float32, device=\"cuda\", requires_grad=True)\n",
    "torch_output = linear(a, b.T, bias=bias).mean()\n",
    "torch_output.backward()\n",
    "\n",
    "print(a.grad[0, :10])\n",
    "print(b.grad[0, :10])\n",
    "print(bias.grad[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a bias to compute the output of a linear layer\n",
    "@triton.autotune(\n",
    "        configs=get_cuda_autotune_config(),\n",
    "        key=['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def nested_linear_expand_kernel(\n",
    "    x_ptr,\n",
    "    w_ptr,\n",
    "    o_ptr,\n",
    "    b_ptr,\n",
    "    mask_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_xm,\n",
    "    stride_xk,\n",
    "    stride_wk,\n",
    "    stride_wn,\n",
    "    stride_om,\n",
    "    stride_on,\n",
    "    stride_bias,\n",
    "    stride_mask,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(BLOCK_SIZE_M, pid_m)\n",
    "    rn = get_1d_offset(BLOCK_SIZE_N, pid_n)\n",
    "    rk = get_1d_offset(BLOCK_SIZE_K, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the m dimension\n",
    "    tmask = tl.load(mask_ptr + rm * stride_mask)\n",
    "    tmask = tl.expand_dims(tmask, 1)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for e_i in range(E): # e is the number of experts\n",
    "        # Get in_dim // 2^(e - e_i)\n",
    "        k_i = K >> (E - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_x = x_ptr + get_2d_offset(rm, rk, stride_xm, stride_xk)\n",
    "        offs_w = w_ptr + get_2d_offset(rk, rn, stride_wk, stride_wn)\n",
    "        \n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        x_mask_i = get_2d_mask(rm, rk, M, k_i)\n",
    "        w_mask_i = get_2d_mask(rk, rn, k_i, N)\n",
    "\n",
    "        # perform dot product for the current expert dimension\n",
    "        for _ in range(0, k_i, BLOCK_SIZE_K):\n",
    "            x = tl.load(offs_x, mask=t_mask_i & x_mask_i, other=0.0)\n",
    "            w = tl.load(offs_w, mask=w_mask_i, other=0.0)\n",
    "            acc += tl.dot(x, w)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_x += BLOCK_SIZE_K * stride_xk\n",
    "            offs_w += BLOCK_SIZE_K * stride_wk\n",
    "\n",
    "    # Add bias after accumulation\n",
    "    if b_ptr is not None:\n",
    "        offs_b = b_ptr + rn * stride_bias\n",
    "        b_mask = rn < N\n",
    "        b = tl.load(offs_b, mask=b_mask, other=0.0)\n",
    "        acc += b[None, :]\n",
    "\n",
    "    o = o_ptr + get_2d_offset(rm, rn, stride_om, stride_on)\n",
    "    mask = get_2d_mask(rm, rn, M, N)\n",
    "    tl.store(o, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the nested linear contraction\n",
    "@triton.autotune(\n",
    "        configs=get_cuda_autotune_config(),\n",
    "        key=['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def nested_linear_contract_kernel(\n",
    "    x_ptr,\n",
    "    w_ptr,\n",
    "    o_ptr,\n",
    "    b_ptr,\n",
    "    mask_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_xm,\n",
    "    stride_xk,\n",
    "    stride_wk,\n",
    "    stride_wn,\n",
    "    stride_om,\n",
    "    stride_on,\n",
    "    stride_bias,\n",
    "    stride_mask,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    # get swizzled program ids\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)\n",
    "\n",
    "    # get offsets for each axis\n",
    "    rm = get_1d_offset(BLOCK_SIZE_M, pid_m)\n",
    "    rn = get_1d_offset(BLOCK_SIZE_N, pid_n)\n",
    "    rk = get_1d_offset(BLOCK_SIZE_K, 0) # K will always start from 0\n",
    "\n",
    "    # token mask only operates on the m dimension\n",
    "    tmask = tl.load(mask_ptr + rm * stride_mask)\n",
    "    tmask = tl.expand_dims(tmask, 1)\n",
    "\n",
    "    # mask for the current expert\n",
    "    x_mask_i = get_2d_mask(rm, rk, M, K)\n",
    "\n",
    "    # initialize accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for e_i in range(E): # e is the number of experts\n",
    "        # Get out_dim // 2^(e - e_i)\n",
    "        n_i = N >> (E - e_i - 1)\n",
    "\n",
    "        # relevant offsets for a and b\n",
    "        # needs to be updated for each expert\n",
    "        offs_x = x_ptr + get_2d_offset(rm, rk, stride_xm, stride_xk)\n",
    "        offs_w = w_ptr + get_2d_offset(rk, rn, stride_wk, stride_wn)\n",
    "\n",
    "        # get masks for the current expert\n",
    "        t_mask_i = (tmask == (e_i))\n",
    "        w_mask_i = get_2d_mask(rk, rn, K, n_i)\n",
    "\n",
    "        for _ in range(0, K, BLOCK_SIZE_K):\n",
    "            x = tl.load(offs_x, mask=t_mask_i & x_mask_i, other=0.0)\n",
    "            w = tl.load(offs_w, mask=w_mask_i, other=0.0)\n",
    "            acc += tl.dot(x, w)\n",
    "\n",
    "            # update offsets for next iteration\n",
    "            offs_x += BLOCK_SIZE_K * stride_xk\n",
    "            offs_w += BLOCK_SIZE_K * stride_wk\n",
    "\n",
    "    # Add bias after accumulation\n",
    "    if b_ptr is not None:\n",
    "        offs_b = b_ptr + rn * stride_bias\n",
    "        b_mask = rn < N\n",
    "        b = tl.load(offs_b, mask=b_mask, other=0.0)\n",
    "        acc += b[None, :]\n",
    "\n",
    "    o = o_ptr + get_2d_offset(rm, rn, stride_om, stride_on)\n",
    "    mask = get_2d_mask(rm, rn, M, N)\n",
    "    tl.store(o, acc, mask=mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_linear_expand(x, w, mask, b=None, experts=4):\n",
    "    assert x.shape[1] == w.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.is_contiguous(), \"Matrix x must be contiguous\"\n",
    "    M, K = x.shape\n",
    "    K, N = w.shape\n",
    "    # Allocate output\n",
    "    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n",
    "    # 1D launch kernel where each block gets its own program\n",
    "    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']), triton.cdiv(N, meta['BLOCK_SIZE_N']))\n",
    "    nested_linear_expand_kernel[grid](\n",
    "        x, w, output, b, mask, M, N, K, experts,\n",
    "        x.stride(0), x.stride(1),\n",
    "        w.stride(0), w.stride(1),\n",
    "        output.stride(0), output.stride(1),\n",
    "        b.stride(0) if b is not None else None,\n",
    "        mask.stride(0),\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 64.,  64.,  64.,  ...,  64.,  64.,  64.],\n",
      "        [ 32.,  32.,  32.,  ...,  32.,  32.,  32.],\n",
      "        [128., 128., 128.,  ..., 128., 128., 128.],\n",
      "        [ 16.,  16.,  16.,  ...,  16.,  16.,  16.],\n",
      "        [ 16.,  16.,  16.,  ...,  16.,  16.,  16.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Test nested_linear_expand while keeping the mask fixed\n",
    "x = torch.ones((5, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "w = torch.ones((128, 256), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "bias = torch.zeros((256), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([2, 1, 3, 0, 0], dtype=torch.int32, device=\"cuda\")\n",
    "output = nested_linear_expand(x, w, mask, bias, experts=4)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 64.,  64.,  64.,  ...,  64.,  64.,  64.],\n",
      "         [ 32.,  32.,  32.,  ...,  32.,  32.,  32.],\n",
      "         [128., 128., 128.,  ..., 128., 128., 128.],\n",
      "         [ 16.,  16.,  16.,  ...,  16.,  16.,  16.],\n",
      "         [ 16.,  16.,  16.,  ...,  16.,  16.,  16.]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# test forward pass with pytorch linear expand\n",
    "from layers import NestedLinearExpand\n",
    "x = torch.ones((1, 5, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([2, 1, 3, 0, 0], dtype=torch.int32, device=\"cuda\").view(1, 5)\n",
    "nested_linear_expand = NestedLinearExpand(128, 256, 4)\n",
    "nested_linear_expand.half()\n",
    "nested_linear_expand.deterministic_init()\n",
    "nested_linear_expand.cuda()\n",
    "output = nested_linear_expand(x, mask)\n",
    "print(output)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_linear_contract(x, w, mask, b=None, experts=4):\n",
    "    assert x.shape[1] == w.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.is_contiguous(), \"Matrix x must be contiguous\"\n",
    "    M, K = x.shape\n",
    "    K, N = w.shape\n",
    "    # Allocate output\n",
    "    output = torch.empty((M, N), device=x.device, dtype=x.dtype)\n",
    "    # 1D launch kernel where each block gets its own program\n",
    "    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE_M']), triton.cdiv(N, meta['BLOCK_SIZE_N']))\n",
    "    nested_linear_contract_kernel[grid](\n",
    "        x, w, output, b, mask, M, N, K, experts,\n",
    "        x.stride(0), x.stride(1),\n",
    "        w.stride(0), w.stride(1),\n",
    "        output.stride(0), output.stride(1),\n",
    "        b.stride(0) if b is not None else None,\n",
    "        mask.stride(0),\n",
    "    )\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "         256., 256., 256., 256., 256., 256., 256., 256.]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test nested_linear_contract while keeping the mask fixed\n",
    "x = torch.ones((5, 256), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "w = torch.ones((256, 128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "bias = torch.zeros((128), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([0, 1, 0, 0, 3], dtype=torch.int32, device=\"cuda\")\n",
    "output = nested_linear_contract(x, w, mask, bias, experts=4)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "            0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "         [256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256., 256., 256., 256., 256.,\n",
      "          256., 256., 256., 256., 256., 256., 256.]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ViewBackward0>)\n",
      "torch.Size([1, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "# test forward pass with pytorch linear contract\n",
    "from layers import NestedLinearContract\n",
    "x = torch.ones((1, 5, 256), device=\"cuda\", dtype=torch.float16, requires_grad=True)\n",
    "mask = torch.tensor([0, 1, 0, 0, 3], dtype=torch.int32, device=\"cuda\").view(1, 5)\n",
    "linear_contract = NestedLinearContract(256, 128, 4)\n",
    "linear_contract.half()\n",
    "linear_contract.deterministic_init()\n",
    "linear_contract.cuda()\n",
    "output = linear_contract(x, mask)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's implement the backward pass for grad_input which is grad_output * w^T\n",
    "# and grad_weight which is x^T * grad_output\n",
    "\n",
    "@triton.jit\n",
    "def nested_linear_expand_dinput_kernel(\n",
    "    grad_output_ptr,\n",
    "    w_ptr,\n",
    "    grad_input_ptr,\n",
    "    mask_ptr,\n",
    "    bias_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_gom,\n",
    "    stride_gon,\n",
    "    stride_gok,\n",
    "    stride_mask,\n",
    "    stride_bias,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pass\n",
    "\n",
    "@triton.jit\n",
    "def nested_linear_contract_dweight_kernel(\n",
    "    grad_output_ptr,\n",
    "    x_ptr,\n",
    "    grad_weight_ptr,\n",
    "    mask_ptr,\n",
    "    bias_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    E,\n",
    "    stride_gom,\n",
    "    stride_gon,\n",
    "    stride_gok,\n",
    "    stride_mask,\n",
    "    stride_bias,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    pass\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import linear\n",
    "\n",
    "class NestedLinearExpandFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w, bias, mask, num_experts):\n",
    "        x = x.view(-1, x.shape[-1])\n",
    "        w = w.transpose()\n",
    "        ctx.save_for_backward(x, w)\n",
    "        output = nested_linear_expand(x, w, mask, bias, num_experts)\n",
    "        return output.view(x.shape[:-1] + (w.shape[-1],))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, w = ctx.saved_tensors\n",
    "        dw = torch.matmul(grad_output, x)\n",
    "        dx = torch.matmul(grad_output, w)\n",
    "        db = grad_output.sum(dim=0)\n",
    "        return dx.view(x.shape), dw.view(w.shape), db\n",
    "    \n",
    "class NestedLinearContractFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w, bias, mask, num_experts):\n",
    "        x = x.view(-1, x.shape[-1])\n",
    "        w = w.transpose()\n",
    "        ctx.save_for_backward(x, w)\n",
    "        output = nested_linear_contract(x, w, mask, bias, num_experts)\n",
    "        return output.view(x.shape[:-1] + (w.shape[-1],))\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.view(-1, grad_output.shape[-1])\n",
    "        x, w = ctx.saved_tensors\n",
    "        dx = torch.matmul(grad_output, w)\n",
    "        dw = torch.matmul(x, grad_output)\n",
    "        db = grad_output.sum(dim=0)\n",
    "        return dx.view(x.shape), dw.view(w.shape), db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for forward expand: 0.00045578479766845704\n",
      "Time for forward contract: 0.0005160093307495118\n"
     ]
    }
   ],
   "source": [
    "# Lets test the backward pass on pytorch versions of the functions\n",
    "from layers import NestedLinearExpand, NestedLinearContract\n",
    "import time\n",
    "\n",
    "x = torch.ones((1, 5, 128), device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
    "token_mask = torch.tensor([2, 1, 3, 0, 0], dtype=torch.int32, device=\"cuda\").view(1, 5)\n",
    "linear_expand = NestedLinearExpand(128, 256, 4)\n",
    "linear_contract = NestedLinearContract(128, 64, 4)\n",
    "linear_contract.deterministic_init()\n",
    "linear_expand.deterministic_init()\n",
    "linear_expand.cuda()\n",
    "linear_contract.cuda()\n",
    "\n",
    "def time_expand():\n",
    "    total = 0\n",
    "    for _ in range(20):\n",
    "        start = time.time()\n",
    "        linear_expand(x, token_mask)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        total += end - start\n",
    "    return total / 20\n",
    "\n",
    "def time_contract():\n",
    "    total = 0\n",
    "    for _ in range(20):\n",
    "        start = time.time()\n",
    "        linear_contract(x, token_mask)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        total += end - start\n",
    "    return total / 20\n",
    "\n",
    "print(f\"Time for forward expand: {time_expand()}\")\n",
    "print(f\"Time for forward contract: {time_contract()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for forward expand triton: 6.181001663208008e-05\n",
      "Time for forward contract triton: 6.07609748840332e-05\n"
     ]
    }
   ],
   "source": [
    "# Lets test the forward pass on the triton versions\n",
    "x = torch.ones((1, 5, 128), device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
    "w1 = torch.ones((128, 256), device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
    "w2 = torch.ones((128, 64), device=\"cuda\", dtype=torch.float32, requires_grad=True)\n",
    "token_mask = torch.tensor([2, 1, 3, 0, 0], dtype=torch.int32, device=\"cuda\").view(1, 5)\n",
    "\n",
    "def time_expand_triton():\n",
    "    total = 0\n",
    "    for _ in range(20):\n",
    "        start = time.time()\n",
    "        nested_linear_expand(x.view(-1, 128), w1, token_mask, None, 4)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        total += end - start\n",
    "    return total / 20\n",
    "\n",
    "def time_contract_triton():\n",
    "    total = 0\n",
    "    for _ in range(20):\n",
    "        start = time.time()\n",
    "        nested_linear_contract(x.view(-1, 128), w2, token_mask, None, 4)\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        total += end - start\n",
    "    return total / 20\n",
    "\n",
    "print(f\"Time for forward expand triton: {time_expand_triton()}\")\n",
    "print(f\"Time for forward contract triton: {time_contract_triton()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
