defaults:
  - model: vit_base
  - _self_
  # Hydra launcher
  # - hydra/launcher: submitit_slurm
  # - hydra/launcher: submitit_local

seed: 42
  
train:
  epochs: 300
  devices: 1
  precision: "bf16-mixed"
  grad_clip: 1.0

# Data configuration
data:
  batch_size: 128
  num_workers: 4
  dataset_name: imagenet1k
  train_path: "path/to/train"
  val_path: "path/to/val"
  dataset_variants:
    imagenet1k:
        num_classes: 1000
        training_size: 1281167
        validation_size: 50000
    imagenet21k:
      num_classes: 21841
      training_size: 14197122
  dataset_selected: ${dataset.dataset_variants.${dataset.name}}
  
# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-3
  weight_decay: 0.05
  
# Learning rate scheduler
scheduler:
  num_warmup_steps: 10
  num_training_steps: ${train.epochs} * ${dataset.dataset_selected.training_size} / ${data.batch_size}

# Logging configuration
logging:
  wandb:
    project: "vit-training"
    name: null  # Will be auto-generated if not specified
    
# Training intervals
gradient_accumulation: 1
log_interval: 50
val_interval: 1
save_interval: 10

# Checkpoint configuration
checkpoints:
  path: "checkpoints"
  resume_from_checkpoint: null 

# MoNE configuration
mone:
  effective_capacity: 1.0
  num_experts: 4
  delta: 2
  beta: 10
  jitter_noise: 0.1

# Augmentation configuration
augmentation:
  mixup: 0.8
  cutmix: 1.0
  random_erase: 0.25
  label_smoothing: 0.1
  randaugment:
    num_ops: 9
    magnitude: 0.5
    num_layers: 2

